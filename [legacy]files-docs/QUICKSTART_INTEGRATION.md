# Quick Start: Integrating Optimized Code
## 5-Minute Integration Guide

---

## Step 1: Backup Your Current Code

```bash
cd /home/kawaiikali/event-study/event_study/code/

# Create backup directory
mkdir -p ../../backups/$(date +%Y%m%d)/

# Backup current implementations
cp tarch_x_manual.py ../../backups/$(date +%Y%m%d)/
cp bootstrap_inference.py ../../backups/$(date +%Y%m%d)/
cp garch_models.py ../../backups/$(date +%Y%m%d)/

echo "Backup created in ../../backups/$(date +%Y%m%d)/"
```

---

## Step 2: Enable Logging (Recommended)

Add this to the top of your analysis script:

```python
import logging

# Configure logging for the entire analysis
logging.basicConfig(
    level=logging.INFO,  # Change to DEBUG for verbose output
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/home/kawaiikali/event-study/event_study_analysis.log'),
        logging.StreamHandler()  # Also print to console
    ]
)

logger = logging.getLogger(__name__)
logger.info("Starting event study analysis...")
```

---

## Step 3: Option A - Side-by-Side Testing (Recommended)

Keep both implementations and test optimized version first:

```python
# In your analysis script
from tarch_x_manual import estimate_tarch_x_manual as estimate_original
from tarch_x_manual_optimized import estimate_tarch_x_manual as estimate_optimized

# Test on single cryptocurrency first
import pandas as pd
from data_preparation import load_and_prepare_single_crypto

# Load BTC data
btc_data = load_and_prepare_single_crypto('btc')

# Prepare exogenous variables
exog_cols = [col for col in btc_data.columns if col.startswith('D_') or 'gdelt' in col]
exog_vars = btc_data[exog_cols]
returns = btc_data['returns_winsorized']

# Run both implementations
logger.info("Testing original implementation...")
original_results = estimate_original(returns, exog_vars)

logger.info("Testing optimized implementation...")
optimized_results = estimate_optimized(returns, exog_vars)

# Compare results
logger.info("Comparing results...")
for param_name in original_results.params.keys():
    orig_val = original_results.params[param_name]
    opt_val = optimized_results.params[param_name]
    diff = abs(orig_val - opt_val)

    if diff > 1e-6:
        logger.warning(f"Parameter {param_name} differs: {diff:.2e}")
    else:
        logger.info(f"âœ“ Parameter {param_name} matches (diff: {diff:.2e})")

logger.info(f"Original time: Check logs for timing")
logger.info(f"Optimized time: Check logs for timing")
```

---

## Step 4: Option B - Direct Replacement

**Only after Step 3 validates results!**

```bash
# Rename original implementation
mv tarch_x_manual.py tarch_x_manual_original.py

# Copy optimized version as main implementation
cp tarch_x_manual_optimized.py tarch_x_manual.py

# Your existing code now uses optimized version automatically!
```

Or update your imports:

```python
# Change this:
from tarch_x_manual import estimate_tarch_x_manual

# To this:
from tarch_x_manual_optimized import estimate_tarch_x_manual
```

---

## Step 5: Update Bootstrap Inference

```python
# Original
from bootstrap_inference import BootstrapInference

bootstrap = BootstrapInference(returns, n_bootstrap=500)
results = bootstrap.residual_bootstrap_tarch(model_type='TARCH')

# Optimized (with parallel execution)
from bootstrap_inference_optimized import BootstrapInference

bootstrap = BootstrapInference(
    returns,
    n_bootstrap=500,
    n_jobs=-1  # Use all CPU cores (NEW!)
)
results = bootstrap.residual_bootstrap_tarch(model_type='TARCH')

logger.info(f"Convergence rate: {results['convergence_rate']:.1%}")
```

---

## Step 6: Full Pipeline Example

Here's a complete analysis using optimized code:

```python
"""
Optimized event study analysis pipeline.
"""

import logging
import pandas as pd
import numpy as np
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('event_study_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Import optimized modules
from data_preparation import DataPreparation
from garch_models import GARCHModels
from tarch_x_manual_optimized import estimate_tarch_x_manual
from bootstrap_inference_optimized import BootstrapInference

def main():
    """Run complete event study analysis with optimized code."""

    logger.info("="*60)
    logger.info("Event Study Analysis - Optimized Pipeline")
    logger.info("="*60)

    # Step 1: Prepare data
    logger.info("Step 1: Data preparation...")
    prep = DataPreparation()
    crypto_data = prep.prepare_all_cryptos(
        include_events=True,
        include_sentiment=True
    )
    logger.info(f"Loaded {len(crypto_data)} cryptocurrencies")

    # Step 2: Estimate models for each cryptocurrency
    logger.info("\nStep 2: Model estimation...")
    all_results = {}

    for crypto, data in crypto_data.items():
        logger.info(f"\n{'='*60}")
        logger.info(f"Analyzing {crypto.upper()}")
        logger.info(f"{'='*60}")

        # GARCH models
        modeler = GARCHModels(data, crypto)
        models = modeler.estimate_all_models()
        all_results[crypto] = models

        # Extract TARCH-X results
        if 'TARCH-X' in models and models['TARCH-X'].convergence:
            tarchx = models['TARCH-X']
            logger.info(f"\n{crypto.upper()} - TARCH-X Results:")
            logger.info(f"  Log-likelihood: {tarchx.log_likelihood:.2f}")
            logger.info(f"  AIC: {tarchx.aic:.2f}")
            logger.info(f"  BIC: {tarchx.bic:.2f}")

            if tarchx.event_effects:
                logger.info(f"\n  Event effects:")
                for event, coef in tarchx.event_effects.items():
                    pval = tarchx.pvalues.get(event, np.nan)
                    sig = "***" if pval < 0.01 else "**" if pval < 0.05 else "*" if pval < 0.10 else ""
                    logger.info(f"    {event}: {coef:+.6f}{sig} (p={pval:.4f})")

    # Step 3: Bootstrap inference (on best model)
    logger.info("\n\nStep 3: Bootstrap inference...")

    # Choose BTC as example
    btc_data = crypto_data['btc']
    returns = btc_data['returns_winsorized']

    # Run bootstrap with parallel execution
    bootstrap = BootstrapInference(
        returns,
        n_bootstrap=500,
        seed=42,
        n_jobs=-1  # All cores
    )

    boot_results = bootstrap.residual_bootstrap_tarch(
        model_type='TARCH',
        include_leverage=True,
        show_progress=True
    )

    logger.info(f"\nBootstrap Results:")
    logger.info(f"  Convergence rate: {boot_results['convergence_rate']:.1%}")
    logger.info(f"  Successful replications: {len(boot_results['bootstrap_params'])}/500")

    # Display confidence intervals
    ci_table = bootstrap.create_bootstrap_table(boot_results)
    logger.info(f"\nBootstrap Confidence Intervals:\n{ci_table.to_string(index=False)}")

    # Step 4: Save results
    logger.info("\n\nStep 4: Saving results...")
    output_dir = Path('/home/kawaiikali/event-study/outputs/analysis_results')
    output_dir.mkdir(parents=True, exist_ok=True)

    # Save model comparison
    comparison_data = []
    for crypto, models in all_results.items():
        for model_name, result in models.items():
            if result.convergence:
                comparison_data.append({
                    'Crypto': crypto.upper(),
                    'Model': model_name,
                    'AIC': result.aic,
                    'BIC': result.bic,
                    'Log-Likelihood': result.log_likelihood
                })

    comparison_df = pd.DataFrame(comparison_data)
    comparison_df.to_csv(output_dir / 'model_comparison.csv', index=False)
    logger.info(f"Saved: {output_dir / 'model_comparison.csv'}")

    # Save bootstrap results
    ci_table.to_csv(output_dir / 'bootstrap_confidence_intervals.csv', index=False)
    logger.info(f"Saved: {output_dir / 'bootstrap_confidence_intervals.csv'}")

    logger.info("\n\n" + "="*60)
    logger.info("Analysis Complete!")
    logger.info("="*60)
    logger.info(f"Results saved to: {output_dir}")
    logger.info(f"Log file: event_study_analysis.log")


if __name__ == "__main__":
    import time
    start_time = time.time()

    main()

    elapsed = time.time() - start_time
    logger.info(f"\nTotal runtime: {elapsed/60:.1f} minutes")
```

---

## Step 7: Verify Performance Improvement

Add timing to your existing code:

```python
import time

# Benchmark original
start = time.time()
original_results = estimate_original(returns, exog_vars)
original_time = time.time() - start

# Benchmark optimized
start = time.time()
optimized_results = estimate_optimized(returns, exog_vars)
optimized_time = time.time() - start

speedup = original_time / optimized_time
logger.info(f"\nPerformance:")
logger.info(f"  Original:  {original_time:.2f}s")
logger.info(f"  Optimized: {optimized_time:.2f}s")
logger.info(f"  Speedup:   {speedup:.2f}x")
```

---

## Step 8: Run Type Checking (Optional but Recommended)

```bash
# Install mypy if not already installed
pip install mypy

# Check optimized files
mypy event_study/code/tarch_x_manual_optimized.py
mypy event_study/code/bootstrap_inference_optimized.py

# Should see: Success: no issues found in X source files
```

---

## Common Issues & Solutions

### Issue 1: Import errors

```python
# Error: ModuleNotFoundError: No module named 'tarch_x_manual_optimized'

# Solution: Check your PYTHONPATH
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))
```

### Issue 2: Logging too verbose

```python
# Solution: Change log level
logging.getLogger('tarch_x_manual_optimized').setLevel(logging.WARNING)
```

### Issue 3: Parallel bootstrap uses too much memory

```python
# Solution: Reduce number of parallel jobs
bootstrap = BootstrapInference(returns, n_bootstrap=500, n_jobs=4)  # Instead of -1
```

### Issue 4: Results differ slightly

```python
# This is expected due to numerical precision
# Acceptable difference: < 1e-6 for parameters

# Check if difference is within tolerance
assert abs(original_results.params['alpha'] - optimized_results.params['alpha']) < 1e-6
```

---

## Rollback Plan (If Needed)

If you encounter issues, easy rollback:

```bash
# Restore original implementation
cd /home/kawaiikali/event-study/event_study/code/
cp ../../backups/$(date +%Y%m%d)/tarch_x_manual.py .
cp ../../backups/$(date +%Y%m%d)/bootstrap_inference.py .

# Or use timestamped backup
ls -lt ../../backups/  # Find your backup date
cp ../../backups/YYYYMMDD/*.py .
```

---

## Performance Monitoring

Track your speedups:

```python
import time
import json
from pathlib import Path

class PerformanceTracker:
    """Track and save performance metrics."""

    def __init__(self, output_file='performance_log.json'):
        self.output_file = Path(output_file)
        self.metrics = []

    def time_operation(self, name, func, *args, **kwargs):
        """Time a function call and log results."""
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start

        self.metrics.append({
            'operation': name,
            'time_seconds': elapsed,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })

        logger.info(f"{name}: {elapsed:.2f}s")
        return result

    def save(self):
        """Save metrics to JSON."""
        with open(self.output_file, 'w') as f:
            json.dump(self.metrics, f, indent=2)

# Usage
tracker = PerformanceTracker()

# Time each operation
data = tracker.time_operation(
    "Data preparation",
    prep.prepare_crypto_data,
    'btc'
)

results = tracker.time_operation(
    "TARCH-X estimation",
    estimate_tarch_x_manual,
    returns,
    exog_vars
)

# Save metrics
tracker.save()
logger.info(f"Performance metrics saved to {tracker.output_file}")
```

---

## Next Steps

1. **Test on single cryptocurrency** (Step 3)
2. **Verify results match** (numerical equivalence)
3. **Integrate into full pipeline** (Step 6)
4. **Monitor performance** (should see 5-8x speedup)
5. **Update documentation** with new timing estimates

---

## Support

If you encounter issues:

1. **Check logs:** `tail -f event_study_analysis.log`
2. **Enable debug logging:** `logging.basicConfig(level=logging.DEBUG)`
3. **Compare results:** Run side-by-side test (Step 3)
4. **Verify installation:** `python -c "from tarch_x_manual_optimized import *"`

---

**Estimated integration time:** 15-30 minutes
**Expected speedup:** 5-8x for full pipeline
**Risk level:** Low (API-compatible, numerically equivalent)

---

Good luck with your research! ðŸš€
